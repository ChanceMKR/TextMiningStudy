{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chance/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading model.safetensors: 100%|██████████| 268M/268M [00:30<00:00, 8.86MB/s]\n",
      "2023-08-10 20:14:22.333796: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 20:14:22.372716: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 279kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 643kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감성분석 결과: POSITIVE, 감성스코어: 0.9999\n"
     ]
    }
   ],
   "source": [
    "clf = pipeline(\"sentiment-analysis\")\n",
    "result = clf(\"what a beautiful day!\")[0]\n",
    "print(f\"감성분석 결과: {result['label']}, 감성스코어: {result['score']:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 1.42MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 548M/548M [01:18<00:00, 7.00MB/s]\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.44MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 840kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 5.16MB/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/chance/opt/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/generation/tf_utils.py:854: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, but I knew that I could find her anywhere on the outside. In the shadows—afterwards—it was difficult finding her.\"\n",
      "\n",
      "A few days later, when\n"
     ]
    }
   ],
   "source": [
    "text_generator = pipeline(\"text-generation\")\n",
    "result = text_generator(\"Alice was beginning to get very tired of sitting by her sister on the bank,\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.6 자동 클래스를 이용한 토크나이저와 모형의 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 433M/433M [01:01<00:00, 7.00MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: 43%\n",
      "yes: 57%\n"
     ]
    }
   ],
   "source": [
    "# Auto Classes를 이용해 사전학습된 내용에 맞는 토크나이저와 모형을 자동으로 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased-finetuned-mrpc\"\n",
    ")\n",
    "\n",
    "# 의미적으로 유사한 두 문장을 선언\n",
    "input_sentence = \"She angered me with her inappropriate comments, rumor-spreading, and disrespectfulness at the formal dinner table\"\n",
    "target_sequence = \"she made me angry when she was rude at dinner\"\n",
    "# 토큰화\n",
    "tokens = tokenizer(input_sentence, target_sequence, return_tensors=\"pt\")\n",
    "\n",
    "# 모형으로 결과를 예측\n",
    "logits = model(**tokens).logits\n",
    "\n",
    "# 소프트맥스를 이용해 결과값을 클래스에 대한 확률로 변환\n",
    "results = torch.softmax(logits, dim=1).tolist()[0]\n",
    "\n",
    "for i, label in enumerate(['no', 'yes']):\n",
    "    print(f\"{label}: {int(round(results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: 95%\n",
      "yes: 5%\n"
     ]
    }
   ],
   "source": [
    "target_sequence = \"The boy quickly ran across the finish line, seizing yet another victory\"\n",
    "tokens = tokenizer(input_sentence, target_sequence, return_tensors=\"pt\")\n",
    "logits = model(**tokens).logits\n",
    "results = torch.softmax(logits, dim=1).tolist()[0]\n",
    "\n",
    "for i, label in enumerate(['no', 'yes']):\n",
    "    print(f\"{label}: {int(round(results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set count:  1600\n",
      "Test set count:  400\n"
     ]
    }
   ],
   "source": [
    "# movei review data에서 file id를 가져옴\n",
    "fileids = movie_reviews.fileids()\n",
    "\n",
    "# file id를 이용해 raw text file을 가져옴\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids]\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids]\n",
    "\n",
    "# label을 0, 1의 값으로 변환\n",
    "label_dict={'pos':1, 'neg':0}\n",
    "y = np.array([label_dict[c] for c in categories])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, y, test_size=0.2, random_state=7)\n",
    "\n",
    "print(\"train set count: \", len(X_train))\n",
    "print('Test set count: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# gpu 사용\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Classes를 이용해 사전학습된 내용에 맞는 토크나이저와 모형을 자동으로 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# 모델을 gpu로 옮겨서 연산을 준비\n",
    "model = model.to(device)\n",
    "\n",
    "batch_size = 10 # 모형으로 한번에 예측할 데이터의 수\n",
    "y_pred = [] # 전체 예측결과를 저장\n",
    "\n",
    "num_batch = len(y_test)//batch_size\n",
    "\n",
    "for i in range(num_batch):\n",
    "    inputs = tokenizer(\n",
    "        X_test[i*batch_size:(i+1)*batch_size],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # 토큰화 결과를 GPU로 이동\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # 모형으로 결과를 예측\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "    # 결과값을 클래스에 대한 확률로 변환\n",
    "    pred = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # 예측결과를 CPU로 가져와서 넘파이로 변환한 후,\n",
    "    # argmax로 확률이 가장 큰 클래스를 선택함\n",
    "    results = pred.cpu().detach().numpy().argmax(axis=1)\n",
    "\n",
    "    # 전체 예측결과에 추가\n",
    "    y_pred.extend(results.tolist())\n",
    "\n",
    "# gpu 메모리를 비움\n",
    "torch.mps.empty_cahce()\n",
    "\n",
    "score = sum(y_test = np.array(y_pred))/len(y_test)\n",
    "print(f\"NLTK 영화리뷰 감성분석 정확도:{score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:43:32) [Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cbfa331c34500466df6bcee3d13d6f1df6471fa5443ed47e9e61f565d6a8d51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
