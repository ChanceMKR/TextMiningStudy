{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁안', '녕', '하세요', '.', '▁반', '갑', '습니다', '.']\n",
      "{'input_ids': [2, 3135, 5724, 7814, 54, 2207, 5337, 6701, 54, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "print(tokenizer.tokenize(\"안녕하세요. 반갑습니다.\"))\n",
    "inputs = tokenizer(\"안녕하세요. 반간습니다.\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set size: 8282\n",
      "#Validation set size: 2761\n",
      "#Test set size: 3682\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "# rating이 6보다 작으면 0(부정), 6 이상이면 긍정으로 라벨 생성\n",
    "y =[0 if rate < 6 else 1 for rate in df.rating]\n",
    "# 데이터셋을 학습, 검증, 평가 데이터셋으로 분리\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(df.review.tolist(), y, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=0)\n",
    "\n",
    "print('#Train set size:', len(X_train))\n",
    "print('#Validation set size:', len(X_val))\n",
    "print('#Test set size:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits,labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Instance and class checks can only be used with @runtime_checkable protocols",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/chance/Desktop/Coding/TextMining/textmining16-2.ipynb 셀 4\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16-2.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m test_dataset \u001b[39m=\u001b[39m OurDataset(test_input, y_test)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16-2.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# 데이터로더 생성\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16-2.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16-2.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m val_loader \u001b[39m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16-2.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/torch/utils/data/dataloader.py:200\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiprocessing_context \u001b[39m=\u001b[39m multiprocessing_context\n\u001b[1;32m    196\u001b[0m \u001b[39m# Arg-check dataset related before checking samplers because we want to\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m# tell users that iterable-style datasets are incompatible with custom\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# samplers first, so that they don't learn that this combo doesn't work\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# after spending time fixing the custom sampler errors.\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(dataset, IterableDataset):\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m=\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable\n\u001b[1;32m    202\u001b[0m     \u001b[39m# NOTE [ Custom Samplers and IterableDataset ]\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[39m# `IterableDataset` does not support custom `batch_sampler` or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[39m# this, and support custom samplers that specify the assignments to\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39m# specific workers.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/typing.py:1497\u001b[0m, in \u001b[0;36m_ProtocolMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__instancecheck__\u001b[39m(\u001b[39mcls\u001b[39m, instance):\n\u001b[1;32m   1490\u001b[0m     \u001b[39m# We need this method for situations where attributes are\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m     \u001b[39m# assigned in __init__.\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1493\u001b[0m         \u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_is_protocol\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m         \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_is_runtime_protocol\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m         \u001b[39mnot\u001b[39;00m _allow_reckless_class_checks(depth\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m   1496\u001b[0m     ):\n\u001b[0;32m-> 1497\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInstance and class checks can only be used with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1498\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39m @runtime_checkable protocols\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1500\u001b[0m     \u001b[39mif\u001b[39;00m ((\u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_is_protocol\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m             _is_callable_members_only(\u001b[39mcls\u001b[39m)) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m             \u001b[39missubclass\u001b[39m(instance\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, \u001b[39mcls\u001b[39m)):\n\u001b[1;32m   1503\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Instance and class checks can only be used with @runtime_checkable protocols"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 토큰화\n",
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Dataset 생성\n",
    "train_dataset = OurDataset(train_input, y_train)\n",
    "val_dataset = OurDataset(val_input, y_val)\n",
    "test_dataset = OurDataset(test_input, y_test)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# KoBERT 사전학습 모형 로드\n",
    "bert_model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "# BERT를 포함한 신경망 모형\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model, token_size, num_labels):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.token_size = token_size\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_model = pretrained_model\n",
    "\n",
    "        # 분류기 정의\n",
    "        self.classifier = torch.nn.Linear(self.token_size, self.num_labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # BERT 모형에 입력을 넣고 출력을 받음\n",
    "        outputs = self.pretrained_model(**inputs)\n",
    "        # BERT 출력에서 CLS 토큰에 해당하는 부분만 가져옴\n",
    "        bert_clf_token = outputs.last_hidden_state[:,0,:]\n",
    "\n",
    "        return self.classifier(bert_clf_token)\n",
    "    \n",
    "# token_size는 BERT 토큰과 동일, bert_model.config.hidden_size로 알 수 있음\n",
    "model = MyModel(bert_model, num_labels=2, token_size=bert_model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# gpu 사용\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# 옵티마이저를 트랜스포머가 제공하는 AdamW로 설정\n",
    "optim = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01) # 가중치 감쇠 설정\n",
    "# 멀티클래스이므로 크로스 엔트로피를 손실 함수로 사용\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 2      # 학습 epochs를 2회로 설정\n",
    "total_training_steps = num_epochs * len(train_loader)\n",
    "# 학습 스케줄러 설정\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optim,\n",
    "                                            num_training_steps=total_training_steps,\n",
    "                                            num_warmup_steps=200)\n",
    "\n",
    "start = time.time() # 시작시간 기록\n",
    "train_loss = 0\n",
    "eval_steps = 500\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #total_epoch_loss = 0 # epoch의 총 loss 초기화\n",
    "    for batch in train_loader:\n",
    "        model.train()   # 학습모드로 전환\n",
    "        optim.zero_grad()   # 그래디언트 초기화\n",
    "\n",
    "        # 배치에서 라벨을 제외한 입력만 추출해 GPU로 복사\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device) # 배치에서 라벨을 추출해 GPU로 복사\n",
    "        outputs = model(inputs)  # 모형으로 결과 예측\n",
    "\n",
    "        # 두 클래스에 대해 예측하고 각각 비교해야 하므로\n",
    "        # labels에 대해 원핫 인코딩을 적용한 후에 손실을 계산\n",
    "        loss = criterion(outputs, F.one_hot(labels, num_classes=2).float())  # loss 계산\n",
    "        train_loss += loss\n",
    "        loss.backward()  # 그래디언트 게산\n",
    "        optim.step()     # 가중치 업데이트\n",
    "        scheduler.step() # 스케줄러 업데이트\n",
    "\n",
    "        step += 1\n",
    "        if step % eval_steps == 0:  # eval_steps마다 경과한 시간과 loss를 출력\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                model.eval()\n",
    "                for batch in val_loader:\n",
    "                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    # loss 계산\n",
    "                    loss = criterion(outputs, F.one_hot(labels, num_classes=2).float())\n",
    "                    val_loss += loss\n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "            avg_train_loss = train_loss / eval_steps\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                'Step %d, elapsed time: %.2f, train loss: %.4f, validation loss: %.4f' % (step, elapsed, avg_train_loss, avg_val_loss)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "    labels = batch['labels'].to(device)\n",
    "\n",
    "    with torch.no_grad():  # 학습할 필요가 없으므로 그레디언트 계산을 끔\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    predictions = torch.argmax(outputs, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=labels)\n",
    "\n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:43:32) [Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cbfa331c34500466df6bcee3d13d6f1df6471fa5443ed47e9e61f565d6a8d51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
