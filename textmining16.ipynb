{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set size: 8282\n",
      "#Validation set size: 2761\n",
      "#Test set size: 3682\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "# rating이 6보다 작으면 0(부정), 6 이상이면 긍정으로 라벨 생성\n",
    "y =[0 if rate < 6 else 1 for rate in df.rating]\n",
    "# 데이터셋을 학습, 검증, 평가 데이터셋으로 분리\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(df.review.tolist(), y, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=0)\n",
    "\n",
    "print('#Train set size:', len(X_train))\n",
    "print('#Validation set size:', len(X_val))\n",
    "print('#Test set size:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits,labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "class OurDataset(Dataset):\n",
    "    def __init__(self, inputs, labels, transform=None, target_transform=None):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: self.transform(val[idx]) for key, val in self.inputs.items()}\n",
    "        item['labels'] = self.target_transform(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions (`list` of `int`): Predicted labels.\n",
       "    references (`list` of `int`): Ground truth labels.\n",
       "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
       "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
       "\n",
       "Returns:\n",
       "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1-A simple example\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.5}\n",
       "\n",
       "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
       "        >>> print(results)\n",
       "        {'accuracy': 3.0}\n",
       "\n",
       "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.8778625954198473}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass TqdmCallback(Callback):\\n    \"\"\"\\n    A callback to display a progress bar using tqdm\\n\\n    Parameters\\n    ----------\\n    tqdm_kwargs : dict, (optional)\\n        Any argument accepted by the tqdm constructor.\\n        See the `tqdm doc <https://tqdm.github.io/docs/tqdm/#__init__>`_.\\n        Will be forwarded to tqdm.\\n\\n    Examples\\n    --------\\n    >>> import fsspec\\n    >>> from fsspec.callbacks import TqdmCallback\\n    >>> fs = fsspec.filesystem(\"memory\")\\n    >>> path2distant_data = \"/your-path\"\\n    >>> fs.upload(\\n            \".\",\\n            path2distant_data,\\n            recursive=True,\\n            callback=TqdmCallback(),\\n        )\\n\\n    You can forward args to tqdm using the ``tqdm_kwargs`` parameter.\\n\\n    >>> fs.upload(\\n            \".\",\\n            path2distant_data,\\n            recursive=True,\\n            callback=TqdmCallback(tqdm_kwargs={\"desc\": \"Your tqdm description\"}),\\n        )\\n    \"\"\"\\n\\n    def __init__(self, tqdm_kwargs=None, *args, **kwargs):\\n        try:\\n            import tqdm\\n\\n            self._tqdm = tqdm\\n        except ImportError as exce:\\n            raise ImportError(\\n                \"Using TqdmCallback requires tqdm to be installed\"\\n            ) from exce\\n\\n        self._tqdm_kwargs = tqdm_kwargs or {}\\n        super().__init__(*args, **kwargs)\\n\\n    def set_size(self, size):\\n        self.tqdm = self._tqdm.tqdm(total=size, **self._tqdm_kwargs)\\n\\n\\n    def relative_update(self, inc=1):\\n        self.tqdm.update(inc)\\n\\n\\n    def __del__(self):\\n        self.tqdm.close()\\n        self.tqdm = None\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class TqdmCallback(Callback):\n",
    "    \"\"\"\n",
    "    A callback to display a progress bar using tqdm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tqdm_kwargs : dict, (optional)\n",
    "        Any argument accepted by the tqdm constructor.\n",
    "        See the `tqdm doc <https://tqdm.github.io/docs/tqdm/#__init__>`_.\n",
    "        Will be forwarded to tqdm.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import fsspec\n",
    "    >>> from fsspec.callbacks import TqdmCallback\n",
    "    >>> fs = fsspec.filesystem(\"memory\")\n",
    "    >>> path2distant_data = \"/your-path\"\n",
    "    >>> fs.upload(\n",
    "            \".\",\n",
    "            path2distant_data,\n",
    "            recursive=True,\n",
    "            callback=TqdmCallback(),\n",
    "        )\n",
    "\n",
    "    You can forward args to tqdm using the ``tqdm_kwargs`` parameter.\n",
    "\n",
    "    >>> fs.upload(\n",
    "            \".\",\n",
    "            path2distant_data,\n",
    "            recursive=True,\n",
    "            callback=TqdmCallback(tqdm_kwargs={\"desc\": \"Your tqdm description\"}),\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tqdm_kwargs=None, *args, **kwargs):\n",
    "        try:\n",
    "            import tqdm\n",
    "\n",
    "            self._tqdm = tqdm\n",
    "        except ImportError as exce:\n",
    "            raise ImportError(\n",
    "                \"Using TqdmCallback requires tqdm to be installed\"\n",
    "            ) from exce\n",
    "\n",
    "        self._tqdm_kwargs = tqdm_kwargs or {}\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def set_size(self, size):\n",
    "        self.tqdm = self._tqdm.tqdm(total=size, **self._tqdm_kwargs)\n",
    "\n",
    "\n",
    "    def relative_update(self, inc=1):\n",
    "        self.tqdm.update(inc)\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        self.tqdm.close()\n",
    "        self.tqdm = None\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /Users/chance/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/chance/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/chance/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안', '##녕', '##하', '##세', '##요', '.', '반', '##갑', '##습', '##니다', '.']\n",
      "{'input_ids': [101, 9521, 118741, 35506, 24982, 48549, 119, 9321, 118610, 119081, 48345, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "print(tokenizer.tokenize(\"안녕하세요. 반갑습니다.\"))\n",
    "inputs = tokenizer(\"안녕하세요. 반갑습니다.\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('input_ids', [101, 9521, 118741, 35506, 24982, 48549, 119, 9321, 118610, 119081, 48345, 119, 102]), ('token_type_ids', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ('attention_mask', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/chance/Desktop/Coding/TextMining'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/chance/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/chance/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# 토큰화\n",
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Dataset 생성\n",
    "train_dataset = OurDataset(train_input, y_train)\n",
    "val_dataset = OurDataset(val_input, y_val)\n",
    "test_dataset = OurDataset(test_input, y_test)\n",
    "\n",
    "# bert-base-multilingual-cased 사전학습 모형으로부터 분류기 모형을 생성\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Instance and class checks can only be used with @runtime_checkable protocols",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb 셀 9\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m'\u001b[39m,         \u001b[39m# 모형 예측이나 체크포인트 출력 폴더, 반드시 필요함\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,             \u001b[39m# 학습 에포크 수\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m,              \u001b[39m# strength of weight decay\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# trainer 객체 생성\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     model \u001b[39m=\u001b[39;49m model,                  \u001b[39m# 학습할 모형\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     args\u001b[39m=\u001b[39;49mtraining_args,             \u001b[39m# 위에서 정의한 학습 매개변수\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtrain_dataset,    \u001b[39m# 학습 데이터셋\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mval_dataset,       \u001b[39m# 검증 데이터셋\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# 미세조정학습 실행\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chance/Desktop/Coding/TextMining/textmining16.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/trainer.py:510\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m train_dataset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m has_length(train_dataset) \u001b[39mand\u001b[39;00m args\u001b[39m.\u001b[39mmax_steps \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    506\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtrain_dataset does not implement __len__, max_steps has to be specified\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    509\u001b[0m     train_dataset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 510\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39;49m(train_dataset, torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mIterableDataset)\n\u001b[1;32m    511\u001b[0m     \u001b[39mand\u001b[39;00m args\u001b[39m.\u001b[39mgroup_by_length\n\u001b[1;32m    512\u001b[0m ):\n\u001b[1;32m    513\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe `--group_by_length` option is only available for `Dataset`, not `IterableDataset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    515\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signature_columns \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/typing.py:1497\u001b[0m, in \u001b[0;36m_ProtocolMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__instancecheck__\u001b[39m(\u001b[39mcls\u001b[39m, instance):\n\u001b[1;32m   1490\u001b[0m     \u001b[39m# We need this method for situations where attributes are\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m     \u001b[39m# assigned in __init__.\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1493\u001b[0m         \u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_is_protocol\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m         \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_is_runtime_protocol\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m         \u001b[39mnot\u001b[39;00m _allow_reckless_class_checks(depth\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m   1496\u001b[0m     ):\n\u001b[0;32m-> 1497\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInstance and class checks can only be used with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1498\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39m @runtime_checkable protocols\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1500\u001b[0m     \u001b[39mif\u001b[39;00m ((\u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_is_protocol\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m             _is_callable_members_only(\u001b[39mcls\u001b[39m)) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m             \u001b[39missubclass\u001b[39m(instance\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, \u001b[39mcls\u001b[39m)):\n\u001b[1;32m   1503\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Instance and class checks can only be used with @runtime_checkable protocols"
     ]
    }
   ],
   "source": [
    "# Trainer에서 사용할 하이퍼 파라미터 지정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',         # 모형 예측이나 체크포인트 출력 폴더, 반드시 필요함\n",
    "    num_train_epochs=2,             # 학습 에포크 수\n",
    "    evaluation_strategy=\"steps\",    # 에포크마다 검증 데이터셋에 대한 평가 지표를 출력\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=8,  # 학습에 사용할 배치 사이즈\n",
    "    per_device_eval_batch_size=16,  # 평가에 사용할 배치 사이즈\n",
    "    warmup_steps=200,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,              # strength of weight decay\n",
    ")\n",
    "\n",
    "# trainer 객체 생성\n",
    "trainer = Trainer(\n",
    "    model = model,                  # 학습할 모형\n",
    "    args=training_args,             # 위에서 정의한 학습 매개변수\n",
    "    train_dataset=train_dataset,    # 학습 데이터셋\n",
    "    eval_dataset=val_dataset,       # 검증 데이터셋\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 미세조정학습 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cbfa331c34500466df6bcee3d13d6f1df6471fa5443ed47e9e61f565d6a8d51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
