{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhpWQmQ0u82P",
        "outputId": "365894be-2482-4c8f-ad9c-8f1d96c86b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZAO8e7quee6",
        "outputId": "4e6be612-1d8f-4a80-e92b-56e8447784d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kobert_tokenizer\n",
            "  Cloning http://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-8uqsymix/kobert-tokenizer_cfe21c07dad0434082a3aca81a7b76a8\n",
            "  Running command git clone --filter=blob:none --quiet http://github.com/SKTBrain/KoBERT.git /tmp/pip-install-8uqsymix/kobert-tokenizer_cfe21c07dad0434082a3aca81a7b76a8\n",
            "  warning: redirecting to https://github.com/SKTBrain/KoBERT.git/\n",
            "  Resolved http://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kobert_tokenizer\n",
            "  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4632 sha256=502c3641baabd438855042c940c94f38a4662e2875cb3baa7b939abe3733df8e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mt6n5n2u/wheels/3a/74/e5/6fbd1409fabeabfc1002f7819ec6b89a239d1273abe10bb544\n",
            "Successfully built kobert_tokenizer\n",
            "Installing collected packages: kobert_tokenizer\n",
            "Successfully installed kobert_tokenizer-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install 'git+http://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocTZbhnru0xi",
        "outputId": "39cf8b7f-ad4b-4c03-cabe-b9b4eacdff9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziu3Zi94vbCs",
        "outputId": "887f76ea-3d87-4199-8c4e-1316db8239a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/244.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.21.0\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiHD-277v5hd",
        "outputId": "979a947f-cbda-4d77-8178-81f042de937c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.14.4 dill-0.3.7 evaluate-0.4.0 multiprocess-0.70.15 responses-0.18.0 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSNJGQR0uIhA",
        "outputId": "742ad394-c608-4561-e2a0-26b64842906a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁안', '녕', '하세요', '.', '▁반', '갑', '습니다', '.']\n",
            "{'input_ids': [2, 3135, 5724, 7814, 54, 2207, 5337, 6701, 54, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "print(tokenizer.tokenize(\"안녕하세요. 반갑습니다.\"))\n",
        "inputs = tokenizer(\"안녕하세요. 반간습니다.\")\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhsNARzOuc2t",
        "outputId": "9c3f61bc-3869-48f0-e479-ebbef5eb36a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Train set size: 8282\n",
            "#Validation set size: 2761\n",
            "#Test set size: 3682\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('/content/daum_movie_review.csv')\n",
        "# rating이 6보다 작으면 0(부정), 6 이상이면 긍정으로 라벨 생성\n",
        "y =[0 if rate < 6 else 1 for rate in df.rating]\n",
        "# 데이터셋을 학습, 검증, 평가 데이터셋으로 분리\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(df.review.tolist(), y, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=0)\n",
        "\n",
        "print('#Train set size:', len(X_train))\n",
        "print('#Validation set size:', len(X_val))\n",
        "print('#Test set size:', len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Odo2kjr_vtQ1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits,labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "class OurDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm0GTaFEv2Gy",
        "outputId": "109c0761-5e5f-4c37-8017-096a0eeabec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertModel\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 토큰화\n",
        "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Dataset 생성\n",
        "train_dataset = OurDataset(train_input, y_train)\n",
        "val_dataset = OurDataset(val_input, y_val)\n",
        "test_dataset = OurDataset(test_input, y_test)\n",
        "\n",
        "# 데이터로더 생성\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# KoBERT 사전학습 모형 로드\n",
        "bert_model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "# BERT를 포함한 신경망 모형\n",
        "class MyModel(torch.nn.Module):\n",
        "    def __init__(self, pretrained_model, token_size, num_labels):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.token_size = token_size\n",
        "        self.num_labels = num_labels\n",
        "        self.pretrained_model = pretrained_model\n",
        "\n",
        "        # 분류기 정의\n",
        "        self.classifier = torch.nn.Linear(self.token_size, self.num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # BERT 모형에 입력을 넣고 출력을 받음\n",
        "        outputs = self.pretrained_model(**inputs)\n",
        "        # BERT 출력에서 CLS 토큰에 해당하는 부분만 가져옴\n",
        "        bert_clf_token = outputs.last_hidden_state[:,0,:]\n",
        "\n",
        "        return self.classifier(bert_clf_token)\n",
        "\n",
        "# token_size는 BERT 토큰과 동일, bert_model.config.hidden_size로 알 수 있음\n",
        "model = MyModel(bert_model, num_labels=2, token_size=bert_model.config.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TEGHydQhv_db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69906185-68ec-41a7-dec2-da33dcfea25d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "<ipython-input-3-ce7cb3001a63>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500, elapsed time: 81.25, train loss: 0.4689, validation loss: 0.3419\n",
            "Step 1000, elapsed time: 159.88, train loss: 0.8565, validation loss: 0.3051\n",
            "Step 1500, elapsed time: 238.30, train loss: 1.1067, validation loss: 0.3136\n",
            "Step 2000, elapsed time: 316.80, train loss: 1.3434, validation loss: 0.2757\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "# gpu 사용\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "# 옵티마이저를 트랜스포머가 제공하는 AdamW로 설정\n",
        "optim = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01) # 가중치 감쇠 설정\n",
        "# 멀티클래스이므로 크로스 엔트로피를 손실 함수로 사용\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 2      # 학습 epochs를 2회로 설정\n",
        "total_training_steps = num_epochs * len(train_loader)\n",
        "# 학습 스케줄러 설정\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer=optim,\n",
        "                                            num_training_steps=total_training_steps,\n",
        "                                            num_warmup_steps=200)\n",
        "\n",
        "start = time.time() # 시작시간 기록\n",
        "train_loss = 0\n",
        "eval_steps = 500\n",
        "step = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    #total_epoch_loss = 0 # epoch의 총 loss 초기화\n",
        "    for batch in train_loader:\n",
        "        model.train()   # 학습모드로 전환\n",
        "        optim.zero_grad()   # 그래디언트 초기화\n",
        "\n",
        "        # 배치에서 라벨을 제외한 입력만 추출해 GPU로 복사\n",
        "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "        labels = batch['labels'].to(device) # 배치에서 라벨을 추출해 GPU로 복사\n",
        "        outputs = model(inputs)  # 모형으로 결과 예측\n",
        "\n",
        "        # 두 클래스에 대해 예측하고 각각 비교해야 하므로\n",
        "        # labels에 대해 원핫 인코딩을 적용한 후에 손실을 계산\n",
        "        loss = criterion(outputs, F.one_hot(labels, num_classes=2).float())  # loss 계산\n",
        "        train_loss += loss\n",
        "        loss.backward()  # 그래디언트 게산\n",
        "        optim.step()     # 가중치 업데이트\n",
        "        scheduler.step() # 스케줄러 업데이트\n",
        "\n",
        "        step += 1\n",
        "        if step % eval_steps == 0:  # eval_steps마다 경과한 시간과 loss를 출력\n",
        "            with torch.no_grad():\n",
        "                val_loss = 0\n",
        "                model.eval()\n",
        "                for batch in val_loader:\n",
        "                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                    labels = batch['labels'].to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    # loss 계산\n",
        "                    loss = criterion(outputs, F.one_hot(labels, num_classes=2).float())\n",
        "                    val_loss += loss\n",
        "                avg_val_loss = val_loss / len(val_loader)\n",
        "            avg_train_loss = train_loss / eval_steps\n",
        "            elapsed = time.time() - start\n",
        "            print(\n",
        "                'Step %d, elapsed time: %.2f, train loss: %.4f, validation loss: %.4f' % (step, elapsed, avg_train_loss, avg_val_loss)\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "model.eval()\n",
        "for batch in test_loader:\n",
        "    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad():  # 학습할 필요가 없으므로 그레디언트 계산을 끔\n",
        "        outputs = model(inputs)\n",
        "\n",
        "    predictions = torch.argmax(outputs, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=labels)\n",
        "\n",
        "metric.compute()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd36IQA18bmz",
        "outputId": "ef9fb264-30c0-4f68-b58f-b86b3e1a1e26"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-ce7cb3001a63>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8720803910917979}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DLTBgn2987i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}