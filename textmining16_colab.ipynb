{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1440,"status":"ok","timestamp":1692361837785,"user":{"displayName":"김성찬","userId":"08576430678344354817"},"user_tz":-540},"id":"AlFJlFFmXC6M","outputId":"8dc0c344-0a20-4825-d87e-f898c72ab448"},"outputs":[{"name":"stdout","output_type":"stream","text":["#Train set size: 8282\n","#Validation set size: 2761\n","#Test set size: 3682\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('/content/daum_movie_review.csv')\n","# rating이 6보다 작으면 0(부정), 6 이상이면 긍정으로 라벨 생성\n","y =[0 if rate < 6 else 1 for rate in df.rating]\n","# 데이터셋을 학습, 검증, 평가 데이터셋으로 분리\n","X_train_val, X_test, y_train_val, y_test = train_test_split(df. review.tolist(), y, random_state=0)\n","X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=0)\n","\n","print('#Train set size:', len(X_train))\n","print('#Validation set size:', len(X_val))\n","print('#Test set size:', len(X_test))"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":13686,"status":"ok","timestamp":1692361851471,"user":{"displayName":"김성찬","userId":"08576430678344354817"},"user_tz":-540},"id":"IYS8JtMaXRFA"},"outputs":[],"source":["import torch\n","import evaluate\n","\n","metric = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits,labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","class OurDataset(torch.utils.data.Dataset):\n","    def __init__(self, inputs, labels):\n","        self.inputs = inputs\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":550,"status":"ok","timestamp":1692361852012,"user":{"displayName":"김성찬","userId":"08576430678344354817"},"user_tz":-540},"id":"egXKUc7yYE8B","outputId":"89a35d26-0d4b-446c-deb5-01383424c23c"},"outputs":[{"name":"stdout","output_type":"stream","text":["['안', '##녕', '##하', '##세', '##요', '.', '반', '##갑', '##습', '##니다', '.']\n","{'input_ids': [101, 9521, 118741, 35506, 24982, 48549, 119, 9321, 118610, 119081, 48345, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","print(tokenizer.tokenize(\"안녕하세요. 반갑습니다.\"))\n","inputs = tokenizer(\"안녕하세요. 반갑습니다.\")\n","print(inputs)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":516},"executionInfo":{"elapsed":1238686,"status":"ok","timestamp":1692363090696,"user":{"displayName":"김성찬","userId":"08576430678344354817"},"user_tz":-540},"id":"Dbt8WV3-YP66","outputId":"9c66557f-eb95-4024-c2a3-adfe590cc4ab"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","<ipython-input-2-ce7cb3001a63>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2072' max='2072' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2072/2072 20:02, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.551400</td>\n","      <td>0.541830</td>\n","      <td>0.753350</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.504800</td>\n","      <td>0.475738</td>\n","      <td>0.809127</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.436400</td>\n","      <td>0.519934</td>\n","      <td>0.796088</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.414700</td>\n","      <td>0.421388</td>\n","      <td>0.823977</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["<ipython-input-2-ce7cb3001a63>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n","<ipython-input-2-ce7cb3001a63>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n","<ipython-input-2-ce7cb3001a63>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n","<ipython-input-2-ce7cb3001a63>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"]},{"data":{"text/plain":["TrainOutput(global_step=2072, training_loss=0.4740480005050718, metrics={'train_runtime': 1206.551, 'train_samples_per_second': 13.728, 'train_steps_per_second': 1.717, 'total_flos': 2689808985606720.0, 'train_loss': 0.4740480005050718, 'epoch': 2.0})"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import BertForSequenceClassification\n","from transformers import Trainer, TrainingArguments\n","\n","# 토큰화\n","train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n","val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n","test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")\n","\n","# Dataset 생성\n","train_dataset = OurDataset(train_input, y_train)\n","val_dataset = OurDataset(val_input, y_val)\n","test_dataset = OurDataset(test_input, y_test)\n","\n","# bert-base-multilingual-cased 사전학습 모형으로부터 분류기 모형을 생성\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","# Trainer에서 사용할 하이퍼 파라미터 지정\n","training_args = TrainingArguments(\n","    output_dir='./results',         # 모형 예측이나 체크포인트 출력 폴더, 반드시 필요함\n","    num_train_epochs=2,             # 학습 에포크 수\n","    evaluation_strategy=\"steps\",    # 에포크마다 검증 데이터셋에 대한 평가 지표를 출력\n","    eval_steps=500,\n","    per_device_train_batch_size=8,  # 학습에 사용할 배치 사이즈\n","    per_device_eval_batch_size=16,  # 평가에 사용할 배치 사이즈\n","    warmup_steps=200,               # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,              # strength of weight decay\n",")\n","\n","# trainer 객체 생성\n","trainer = Trainer(\n","    model = model,                  # 학습할 모형\n","    args=training_args,             # 위에서 정의한 학습 매개변수\n","    train_dataset=train_dataset,    # 학습 데이터셋\n","    eval_dataset=val_dataset,       # 검증 데이터셋\n","    compute_metrics=compute_metrics,\n",")\n","\n","# 미세조정학습 실행\n","trainer.train()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3735,"status":"ok","timestamp":1692363105360,"user":{"displayName":"김성찬","userId":"08576430678344354817"},"user_tz":-540},"id":"RGdsiu9Sk63M"},"outputs":[],"source":["trainer.save_model(\"my_model\")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":196},"executionInfo":{"elapsed":67181,"status":"ok","timestamp":1692363221099,"user":{"displayName":"김성찬","userId":"08576430678344354817"},"user_tz":-540},"id":"F7qnTFPEs5H_","outputId":"9c098295-6c73-43bb-f0fc-afe6d48f13f2"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-2-ce7cb3001a63>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='231' max='231' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [231/231 01:06]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 0.4463678300380707,\n"," 'eval_accuracy': 0.8147745790331342,\n"," 'eval_runtime': 66.861,\n"," 'eval_samples_per_second': 55.069,\n"," 'eval_steps_per_second': 3.455,\n"," 'epoch': 2.0}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate(eval_dataset=test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJA8xMXNtDKq"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP9egddh7HwOTrRGnRa6PPG","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"tf","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:43:32) [Clang 12.0.1 ]"},"vscode":{"interpreter":{"hash":"5cbfa331c34500466df6bcee3d13d6f1df6471fa5443ed47e9e61f565d6a8d51"}}},"nbformat":4,"nbformat_minor":0}
